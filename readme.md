LiDAR based SLAM

Simultaneous Localization and Mapping (SLAM) is a crucial capability for autonomous robots, enabling them to navigate and map unknown environments without external posi- tioning systems. This project focuses on implementing a LiDAR- based SLAM system for a differential-drive robot equipped with wheel encoders, an Inertial Measurement Unit (IMU), a 2D LiDAR scanner, and an RGBD camera. The methodology begins with odometry estimation using encoder and IMU data, employ- ing the differential-drive motion model. To refine localization accuracy, LiDAR scan matching is performed using the Iterative Closest Point (ICP) algorithm, which corrects incremental pose errors. The SLAM framework then constructs a 2D occupancy grid map from LiDAR data, while RGBD images are processed to generate a texture-mapped floor representation. Further im- provements in trajectory estimation are achieved through pose graph optimization with loop closure constraints, implemented via the GTSAM library. Results demonstrated the effectiveness of scan matching and enhancing map accuracy, while pose graph optimization further refines localization consistency. This work highlights the integration of multiple sensor modalities for robust SLAM, contributing to reliable autonomous navigation in complex environments.
